{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IbmPtrkCwtM"
   },
   "source": [
    "# HAN for Text Classification\n",
    "\n",
    "\n",
    "*   Load Libraries\n",
    "*   Connect to Google Drive\n",
    "*   Code to clean the text\n",
    "*   Load Data\n",
    "*   Set Parameters\n",
    "*   Clean and prepare train and test data\n",
    "*   Load GloVe embedding model\n",
    "*   Model Architecture\n",
    "*   Fit Model\n",
    "*   Store Model\n",
    "*   Evaluate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12LpxneYC9Vw"
   },
   "source": [
    "# Load all Libraries and download nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7y52CD8tZmXo"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey6bOCRHDA-4"
   },
   "source": [
    "# Connect to Google Drive\n",
    "\n",
    "Use the URL to connect to google drive and give premission for colab to access Goole drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7G5lTf3ZpzV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#Check GPU\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sV_PVwRsDEkB"
   },
   "source": [
    "# Code to Clean the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gz3mCrsZZ_Zl"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4Zzq4G2DF_k"
   },
   "source": [
    "# Load Data\n",
    "\n",
    "Uncomment the dataset that you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12Rall8KaBU6"
   },
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "dataset_name = 'IMDB'\n",
    "#dataset_name = 'AG_News'\n",
    "#dataset_name = 'Amazon'\n",
    "\n",
    "\n",
    "if dataset_name == 'IMDB':\n",
    "  train_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/train.csv\")\n",
    "  test_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/test.csv\")\n",
    "  \n",
    "if dataset_name == 'AG_News':\n",
    "  train_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/train.csv\")\n",
    "  test_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/test.csv\")\n",
    "  train_df['label'] = train_df['label'] -1\n",
    "  test_df['label'] = test_df['label'] -1\n",
    "  \n",
    "if dataset_name == 'Amazon':\n",
    "  df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/Amazon_Data.csv\")\n",
    "  stratsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "  for train_index, test_index in stratsplit.split(df, df['label']):\n",
    "    train_df = df.loc[train_index]\n",
    "    test_df = df.loc[test_index]\n",
    "    train_df.dropna(subset = ['text'],inplace=True)\n",
    "    test_df.dropna(subset = ['text'],inplace=True)\n",
    "    train_df.reset_index(drop=True,inplace=True)\n",
    "    test_df.reset_index(drop=True,inplace=True)\n",
    "    train_df['label'] = train_df['label'] -1\n",
    "    test_df['label'] = test_df['label'] -1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtZc4ajCDKq_"
   },
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBMnISGeeYYi"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.text.apply(lambda x: x !=\"\")]\n",
    "test_df = test_df[test_df.text.apply(lambda x: x !=\"\")]\n",
    "train_df['text'] = train_df['text'].map(lambda x: clean_text(x))\n",
    "test_df['text'] = test_df['text'].map(lambda x: clean_text(x))\n",
    "num_classes = test_df['label'].nunique()\n",
    "list_sentences_train = train_df[\"text\"].fillna(\"_na_\").values\n",
    "train_y = to_categorical(train_df['label'].values,num_classes=num_classes)\n",
    "list_sentences_test = test_df[\"text\"].fillna(\"_na_\").values\n",
    "test_y = to_categorical(test_df['label'].values,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ck4mfe3mDTUr"
   },
   "source": [
    "# Loading and processing GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugQe5HJk9_Ml"
   },
   "outputs": [],
   "source": [
    "# Loading word embedding\n",
    "embeddings_index = dict()\n",
    "f = open('drive/My Drive/Data/GloVe/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "data_test = pad_sequences(sequences_test, maxlen=50)\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSElRCh_DaMm"
   },
   "source": [
    "# Create Embedding metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ualp0xowFAwf"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2YfEesuDhWg"
   },
   "source": [
    "# Convolutional Neural Network (CNN) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqM9hN9QJF-G"
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False)) #set trainbale to true and check\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFLYJk8nDwfF"
   },
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNQZY0ltJI5r"
   },
   "outputs": [],
   "source": [
    "history = model.fit(data, train_y, epochs=1, validation_split=0.2, batch_size=32, callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.01)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_GRHZ2XD2Ev"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHrGLXj8u3ia"
   },
   "outputs": [],
   "source": [
    "# Saving the model and its histort\n",
    "model.save('drive/My Drive/Data/' + dataset_name + '/CNN.h5')\n",
    "data_path = 'drive/My Drive/Data/' + dataset_name\n",
    "with open(data_path+'/trainHistoryDict_CNN.pkl', 'wb') as file_pi:\n",
    "   pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jjhr-pFcEIcu"
   },
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcrP00J7JbJ8"
   },
   "outputs": [],
   "source": [
    "model.evaluate(data_test, test_y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "commented_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
