{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jStSlwVqb3E"
   },
   "source": [
    "# HAN for Text Classification\n",
    "\n",
    "\n",
    "*   Load Libraries\n",
    "*   Connect to Google Drive\n",
    "*   Code to clean the text\n",
    "*   Load Data\n",
    "*   Set Parameters\n",
    "*   Clean and prepare train and test data\n",
    "*   Load GloVe embedding model\n",
    "*   Attention Layer Implementation\n",
    "*   Model Architecture\n",
    "*   Fit Model\n",
    "*   Store Model\n",
    "*   Evaluate Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQQE2J_2rXbF"
   },
   "source": [
    "# Load all Libraries and download nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdErrzcsU0Yp"
   },
   "outputs": [],
   "source": [
    "# All Required imports\n",
    "import string\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D, MaxPooling1D, TimeDistributed\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D, Layer, Embedding, Bidirectional, GRU\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers, constraints\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Download nltk packages for text cleaning\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXfAw3GLrg1Z"
   },
   "source": [
    "# Connect to Google Drive\n",
    "\n",
    "Use the URL to connect to google drive and give premission for colab to access Goole drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TLjNAVRlZiZ"
   },
   "outputs": [],
   "source": [
    "# Loading Google Srive and checking GPU access\n",
    "drive.mount('/content/drive')\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5AwlJl5rwXg"
   },
   "source": [
    "# Code to Clean the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2vjHBwAdODO"
   },
   "outputs": [],
   "source": [
    "# Text cleaning code\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    This function is responsible for preprocessing the reviews before being fed to train the model\n",
    "    1. Remove punctuation\n",
    "    2. Convert all texts to lowercase\n",
    "    3. Remove english stop words\n",
    "    4. Remove special characters\n",
    "    5. Replace common abbreviations with their full forms\n",
    "    '''\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCzyLW7ktM7X"
   },
   "source": [
    "# Load Data\n",
    "\n",
    "Uncomment the dataset that you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3u4U9Urd111"
   },
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "#dataset_name = 'IMDB'\n",
    "#dataset_name = 'AG_News'\n",
    "dataset_name = 'Amazon'\n",
    "\n",
    "\n",
    "if dataset_name == 'IMDB':\n",
    "  train_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/train.csv\")\n",
    "  test_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/test.csv\")\n",
    "  \n",
    "if dataset_name == 'AG_News':\n",
    "  train_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/train.csv\")\n",
    "  test_df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/test.csv\")\n",
    "  train_df['label'] = train_df['label'] -1\n",
    "  test_df['label'] = test_df['label'] -1\n",
    "  \n",
    "if dataset_name == 'Amazon':\n",
    "  df = pd.read_csv(\"drive/My Drive/Data/\" + dataset_name + \"/Amazon_Data.csv\")\n",
    "  stratsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "  for train_index, test_index in stratsplit.split(df, df['label']):\n",
    "    train_df = df.loc[train_index]\n",
    "    test_df = df.loc[test_index]\n",
    "    train_df.dropna(subset = ['text'],inplace=True)\n",
    "    test_df.dropna(subset = ['text'],inplace=True)\n",
    "    train_df.reset_index(drop=True,inplace=True)\n",
    "    test_df.reset_index(drop=True,inplace=True)\n",
    "    train_df['label'] = train_df['label'] -1\n",
    "    test_df['label'] = test_df['label'] -1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIvAu84tr0oo"
   },
   "source": [
    "# Setting up the parameters for embedding and HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nbh0rc01SuwT"
   },
   "outputs": [],
   "source": [
    "# Setting Parameters for HAN models\n",
    "MAX_SENTENCE_NUM = 100\n",
    "MAX_WORD_NUM = 100\n",
    "MAX_FEATURES = 200000 \n",
    "n_rows=train_df.shape[0]\n",
    "n_classes = train_df.label.nunique()\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44oH1sXSr9s4"
   },
   "source": [
    "# Creating train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tKcnwGsT9om"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenize the sentences using keras preprocessing for Training Data,\n",
    "then use those tokens to create the data for training and testing\n",
    "'''\n",
    "\n",
    "# preparing Train data\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "for idx in range(train_df.text.shape[0]):\n",
    "    s = train_df['text'].iloc[idx]\n",
    "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    text = clean_text(s)\n",
    "    texts.append(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "    labels.append(train_df.label[idx])\n",
    "    \n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS,lower=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of tokens: ' + str(len(word_index)))\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data\n",
    "y_train = labels\n",
    "\n",
    "# preparing Test data\n",
    "test_reviews = []\n",
    "test_labels = []\n",
    "test_texts = []\n",
    "\n",
    "for idx in range(test_df.text.shape[0]):\n",
    "    s = test_df['text'].iloc[idx]\n",
    "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    text = clean_text(s)\n",
    "    test_texts.append(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    test_reviews.append(sentences)\n",
    "\n",
    "    test_labels.append(test_df.label[idx])\n",
    "\n",
    "\n",
    "test_data = np.zeros((len(test_texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(test_reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                        test_data[i, j, k] = tokenizer.word_index[word]\n",
    "                        k = k + 1\n",
    "                except:\n",
    "                    test_data[i, j, k] = 0\n",
    "                    k = k + 1\n",
    "                    continue\n",
    "\n",
    "test_labels = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', test_labels.shape)\n",
    "\n",
    "indices = np.arange(test_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "test_data = test_data[indices]\n",
    "test_labels = test_labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwHUeFJ7sDtU"
   },
   "source": [
    "# Loading and processing GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fF6wl6_rVo02"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating work embedding metrix using GloVe model. This is acheived using a pre-trained\n",
    "GloVe model(6 Billion, 100 Dimentions).\n",
    "'''\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('drive/My Drive/Data/GloVe/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "min_wordCount = 2\n",
    "absent_words = 0\n",
    "small_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "word_counts = tokenizer.word_counts\n",
    "for word, i in word_index.items():\n",
    "    if word_counts[word] > min_wordCount:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oJbJGzBsIuU"
   },
   "source": [
    "# Attention Layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OT176g_acm-M"
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "def dot_product(x, kernel):\n",
    "    '''\n",
    "    calculating dot product between 2 metrix\n",
    "    '''\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    '''\n",
    "    The Attention layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, attention_dim=100,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "        Building the Attention layer \n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((self.attention_dim, input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name)\n",
    "                                 )\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.attention_dim,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name))\n",
    "\n",
    "        self.u = self.add_weight((self.attention_dim,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name))\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0UzXhLFsM1T"
   },
   "source": [
    "# Hierarchical Attention Network(HAN) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQTWQfd3cnBA"
   },
   "outputs": [],
   "source": [
    "# HAN model\n",
    "'''\n",
    "This is the hierarchical attention network. This include 2 layers of network,\n",
    "one with work level attention and another is sentence level attention.\n",
    "\n",
    "The first layer contains a word embedding layer, a bi-directional GRU layer, a \n",
    "dropout layer, a word level attention layer and finally an encoder.\n",
    "\n",
    "In the second layer, we have the input layer(taking input from previous layer), \n",
    "then a Time distributed layer,  a bi-directional GRU layer, a \n",
    "dropout layer, a sentence level attention layer and a fully connected dense layer\n",
    "for output.\n",
    "'''\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dropout = Dropout(0.5)(l_lstm)\n",
    "l_att = AttentionLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dropout_1 = Dropout(0.5)(l_lstm_sent)\n",
    "l_att_sent = AttentionLayer(100)(l_lstm_sent)\n",
    "preds = Dense(n_classes, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\t\n",
    "data_path=\"drive/My Drive/Data/\" + dataset_name\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + 'han-model-{epoch:02d}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19PEE-basaFf"
   },
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbZeRUk9tqfd"
   },
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, nb_epoch=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZgYxGCisgV_"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYbgH0tIEmjZ"
   },
   "outputs": [],
   "source": [
    "# Saving the model and its histort\n",
    "model.save('drive/My Drive/Data/' + dataset_name + '/HAN.h5')\n",
    "data_path = 'drive/My Drive/Data/' + dataset_name\n",
    "with open(data_path+'/trainHistoryDict_HAN.pkl', 'wb') as file_pi:\n",
    "   pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9DtPaKHsc7e"
   },
   "source": [
    "# Evaluate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8c03zi5I67UP"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "model.evaluate(test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Commented HAN_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
