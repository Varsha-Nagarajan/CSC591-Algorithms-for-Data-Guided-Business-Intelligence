{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBzmaCaXgRfB"
   },
   "source": [
    "# LSTM and Bi-LSTM Networks for Text Classification \n",
    "\n",
    "\n",
    "*   Import the necessary packages\n",
    "*   Load the data\n",
    "*   Preprocess the data\n",
    "*   Perform train, validation and test splits\n",
    "*   Import pre-trained GloVe embeddings and construct the embedding matrix\n",
    "*   Define the model\n",
    "*   Initialize callbacks like early stopping, checkpoint\n",
    "*   Fit the model on the train set\n",
    "*   Evaluate the model accuracy on test set\n",
    "*   Plot training and validation accuracy and loss\n",
    "\n",
    "### Appendix\n",
    "\n",
    "*   Code snippet used for extracting different raw data into csv formats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJtFsx6DduBg"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJulcIKxS7Fz"
   },
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "rPB8BxjrULO6",
    "outputId": "c7bceb58-be4b-45f1-d3b1-32a731b591d6"
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#others\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCIi5MOpTBPl"
   },
   "source": [
    "### Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-K1B-hhOVNO0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function is responsible for preprocessing the reviews before being fed to train the model\n",
    "1. Remove punctuation\n",
    "2. Convert all texts to lowercase\n",
    "3. Remove english stop words\n",
    "4. Remove special characters\n",
    "5. Replace common abbreviations with their full forms\n",
    "'''\n",
    "def clean_text(text, remove_stopwords=True, stem_words=False, expand_abbreviations=True):\n",
    "    \n",
    "    ## Removing the punctuations (for python 3.0+ versions)\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    # If using python 2.7, uncomment the below line to remove punctuations\n",
    "    #text = ' '.join([word.strip(string.punctuation) for word in text.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove english stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [word for word in text if not word in stops and len(word) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text by removing special characters and replacing popular abbreviations with actual words\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)    \n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\omg\", \" oh my god \", text)\n",
    "    text = re.sub(r\"\\'lol\", \" laughing out loud \", text)\n",
    "    text = re.sub(r\"\\'ok\", \" okay \", text)\n",
    "    \n",
    "    # Stemming\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ege5O_xOxyy"
   },
   "source": [
    "### Indicate dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAQ7VdkXOx8u"
   },
   "outputs": [],
   "source": [
    "dataset_type = \"imdb\"\n",
    "#dataset_type = \"agnews\"\n",
    "#dataset_type = \"amazon\"\n",
    "\n",
    "data_path = 'drive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeR-gF6jTHO8"
   },
   "source": [
    "### Loading Dataset (IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pjg4x0MnWkYD"
   },
   "outputs": [],
   "source": [
    "if dataset_type == \"imdb\":\n",
    "    train_path = data_path + \"Data/IMDB/train.csv\"\n",
    "    test_path = data_path + \"Data/IMDB/test.csv\"\n",
    "\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvOAhLDlbRRu"
   },
   "source": [
    "### Loading Dataset (AG-NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4T7l-pzbReA"
   },
   "outputs": [],
   "source": [
    "if dataset_type == \"agnews\":\n",
    "    train_path = data_path + \"Data/AG_News/train.csv\"\n",
    "    test_path = data_path + \"Data/AG_News/test.csv\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "    # All labels start from 1 hence subtracting it by 1 so as to perform one-hot encoding later\n",
    "    train['label'] = train['label'] - 1\n",
    "    test['label'] = test['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoqjfXW9bRsE"
   },
   "source": [
    "### Loading Dataset (Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXFjp8uHbR67"
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def load_amazon(path):\n",
    "                \n",
    "    df = getDF(path + 'reviews_Toys_and_Games_5.json.gz')\n",
    "    return df\n",
    "\n",
    "if dataset_type == \"amazon\":\n",
    "    data = load_amazon(data_path + \"Data/Amazon/\")\n",
    "    data.columns = ['reviewerID', 'asin', 'reviewerName', 'helpful', 'text', 'label',\n",
    "           'summary', 'unixReviewTime', 'reviewTime']\n",
    "\n",
    "\n",
    "    stratsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in stratsplit.split(data, data['label']):\n",
    "        train = data.loc[train_index]\n",
    "        test = data.loc[test_index]\n",
    "        break\n",
    "\n",
    "    # All labels start from 1 hence subtracting it by 1 so as to perform one-hot encoding later\n",
    "    train['label'] = train['label'] - 1\n",
    "    test['label'] = test['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELb8-F0zTpkN"
   },
   "source": [
    "### Perform data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hR72DUJVetiJ"
   },
   "outputs": [],
   "source": [
    "train = train[train.text.apply(lambda x: x != \"\")]\n",
    "\n",
    "# calling the preprocessing functiond defined above on all the reviews\n",
    "train['text'] = train['text'].map(lambda x: clean_text(x))\n",
    "test['text'] = test['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxasujUqT_X8"
   },
   "source": [
    "### Prepare train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVYHlnEtbi7C"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.asarray(train['label']))\n",
    "y_test = to_categorical(np.asarray(test['label'])) \n",
    "\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzQtEg-BUobH"
   },
   "source": [
    "### Configure parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bHZczPLWOqW"
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 15000\n",
    "\n",
    "# Since amazon reviews are huge and richer, we use 20000 words in the vocabulary.\n",
    "if dataset_type == \"amazon\":\n",
    "    VOCABULARY_SIZE = 20000\n",
    "\n",
    "MAX_REVIEW_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD6veimYUyo5"
   },
   "source": [
    "### Tokenize and Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "27VQ02flUy6B",
    "outputId": "9188e9f8-a522-492b-ec2d-3567e827e7d5"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(train['text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test['text'])\n",
    "\n",
    "# Padding to make all sentences of same length\n",
    "data_test = pad_sequences(sequences_test, maxlen = MAX_REVIEW_LENGTH)\n",
    "data_train = pad_sequences(sequences_train, maxlen = MAX_REVIEW_LENGTH)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpV_HckoU8AZ"
   },
   "source": [
    "### Import GloVe pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AetwXM_rWOsR",
    "outputId": "54c44603-c560-4ecb-93a5-7c167bb47563"
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(data_path + 'Data/GloVe/glove.6B.100d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKX80iIRVO-q"
   },
   "source": [
    "### Create the embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6IWvyKcWOm4"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_DIM))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > VOCABULARY_SIZE - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2izp-NvsVXJU"
   },
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "_i66ZvFuXn-k",
    "outputId": "0dc88172-c6e6-4fbd-f70e-33f1bbc3a091"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_REVIEW_LENGTH, trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 100))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Used for IMDB dataset which is a binary classification usecase\n",
    "# model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Z80phwxWQMk"
   },
   "source": [
    "### Bi-Directional LSTM Model (uncomment if you need to run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3ddcR-vWPMt"
   },
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_REVIEW_LENGTH, trainable=False)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Bidirectional(LSTM(units = 100)))\n",
    "# model.add(Dropout(rate = 0.2))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Used for IMDB dataset which is a binary classification usecase\n",
    "# model.add(Dense(num_classes, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-OkY7eEX2_p"
   },
   "source": [
    "### Define callbacks: early stopping and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXYFR7vgX3Kr"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(data_path + 'model.{epoch:02d}.hdf5', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVz5HiO_WaNa"
   },
   "source": [
    "### Fit the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKEvcN3t-ko7"
   },
   "outputs": [],
   "source": [
    "callbacks = model.fit(data_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size = BATCH_SIZE, callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZTf8AOXWkqy"
   },
   "source": [
    "### Save the final model and the history for plotting graphs later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEJ8EQGCWk5r"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save(data_path + \"final_model.hdf5\")\n",
    "with open(data_path + 'trainValHistoryDict', 'wb') as file_pi:\n",
    "    pickle.dump(callbacks.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENyU63qjgXka"
   },
   "source": [
    "### Run this cell if you want to load an already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GknZEs3HgYKK"
   },
   "outputs": [],
   "source": [
    "model_path = data_path + \"final_model.hdf5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qdNfEjXWlLA"
   },
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaogFyKBtygS"
   },
   "outputs": [],
   "source": [
    "model.evaluate(data_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac2W-lHSW8IF"
   },
   "source": [
    "### Plot the accuracy and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKfuG3fyJD2l"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "num_epochs = 20\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(range(1, num_epochs + 1, 1))\n",
    "y = range(1, num_epochs + 1)\n",
    "c1 = plt.plot(y, np.squeeze(callbacks.history['loss']), color=\"teal\", label=\"Training\")\n",
    "c2 = plt.plot(y, np.squeeze(callbacks.history['val_loss']), color=\"orange\", label=\"Validation\")\n",
    "ax.legend()\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.savefig(data_path + \"image.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hBGP76qW5OZ"
   },
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzLyKbj1Zlum"
   },
   "source": [
    "### Code snippet that was used to extract raw IMDB data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPJWuhJeZkOq"
   },
   "outputs": [],
   "source": [
    "def load_data(path, output_path):\n",
    "    \n",
    "    indices = []\n",
    "    text = []\n",
    "    label = []\n",
    "\n",
    "    i = 0 \n",
    "    \n",
    "    train_pos_path = os.path.join(path, \"pos\")\n",
    "    train_neg_path = os.path.join(path, \"neg\")\n",
    "    \n",
    "    for filename in os.listdir(train_pos_path):\n",
    "        data = open(os.path.join(train_pos_path, filename), 'r').read()\n",
    "        print(data)\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        label.append(\"1\")\n",
    "        i = i + 1\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for filename in os.listdir(train_neg_path):\n",
    "        data = open(os.path.join(train_neg_path, filename), 'r').read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        label.append(\"0\")\n",
    "        i = i + 1\n",
    "\n",
    "    Dataset = list(zip(indices, text, label))\n",
    "\t\n",
    "    # We shuffle it as it is a sequence of postives followed by negatives currently. \n",
    "    np.random.shuffle(Dataset)\n",
    "\n",
    "    df = pd.DataFrame(data = Dataset, columns=['id', 'text', 'label'])\n",
    "    df.to_csv(output_path+\"_Data\", index=False, header=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGJB1phXfiFH"
   },
   "source": [
    "### Code snippet that was used to extract raw Amazon review data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0vqutnQfiPr"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def load_amazon(path):\n",
    "                \n",
    "    df = getDF(path + 'reviews_Toys_and_Games_5.json.gz')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOIybbMQcihy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-Bi-LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
